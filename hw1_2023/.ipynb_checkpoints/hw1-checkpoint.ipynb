{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS145 Howework 1 \n",
    "\n",
    "<span style=\"color:red\"> **Important Note:** </span>\n",
    "The submission deadline for all homeworks are one week from its release date.\n",
    "HW1 is due on **11:59 PM PT, April 19 (Wednesday)**. Please submit through GradeScope (you will receive an invitation for CS145 Spring 2023).\n",
    "\n",
    "## Before You Start\n",
    "\n",
    "You need to first create HW1 conda environment using `cs145hw1.yml`. This file provides the env name and necessary packages for this tasks. If you have `conda` installed, you may create, activate and deactivate an environment using the following commands:\n",
    "\n",
    "```\n",
    "conda create -f cs145hw1.yml\n",
    "conda activate hw1\n",
    "conda deactivate\n",
    "```\n",
    "Here are some references about conda: [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html).\n",
    "\n",
    "You should not delete any code cells in this notebook. If you change any code outside the blocks that you are allowed to edit (between `STRART/END YOUR CODE HERE`), you will need to highlight these changes. You may add additional cells to help explain your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sys\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you can successfully run the code above, there will be no problem for environment setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear regression \n",
    "This example will walk you through three optimization algorithms for linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 100)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "# As a sanity check, we print out the size of the training data (1000, 100) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Closed form solution\n",
    "In this section, complete the `getBeta` function in `linear_regression.py`, which compute the close form solution of $\\hat{\\beta}$.\n",
    "\n",
    "To train you modelm use `lm.train('0')` function.\n",
    "\n",
    "Compute the training error and the testing error using `lm.predict` and `lm.compute_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  0\n",
      "getBeta p: 101\n",
      "Training error is:  0.08693886675396784\n",
      "Testing error is:  0.11017540281675801\n"
     ]
    }
   ],
   "source": [
    "from hw1code.linear_regression import LinearRegression\n",
    "\n",
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "## hint, for training error, you should get something around 0.08\n",
    "lm.normalize()\n",
    "beta = lm.train('0')\n",
    "\n",
    "training_error = lm.compute_mse(lm.predict(lm.train_x, beta), lm.train_y)\n",
    "testing_error = lm.compute_mse(lm.predict(lm.test_x, beta), lm.test_y)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.Batch gradient descent\n",
    "In this section, complete the `getBetaBatchGradient` function in `linear_regression.py`, which computes the gradient of the objective fuction.\n",
    "\n",
    "To train you model, use `lm.train('1')` function.\n",
    "\n",
    "Compute the training error and the testing error using `lm.predict` and `lm.compute_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  1\n",
      "Training error is:  0.09654126176914339\n",
      "Testing error is:  0.1310340968975749\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "lm.normalize()\n",
    "beta = lm.train('1')\n",
    "\n",
    "training_error = lm.compute_mse(lm.predict(lm.train_x, beta), lm.train_y)\n",
    "testing_error = lm.compute_mse(lm.predict(lm.test_x, beta), lm.test_y)\n",
    "\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.Stochastic gadient descent \n",
    "In this section, complete the `getBetaStochasticGradient` function in `linear_regression.py`, which computes an estimated gradient of the objective function.\n",
    "\n",
    "To train you model, use `lm.train('2')` function.\n",
    "\n",
    "Compute the training error and the testing error using `lm.predict` and `lm.compute_mse`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Algorithm Type:  1\n",
      "Training error is:  0.09849656388555593\n",
      "Testing error is:  0.13322649378230336\n"
     ]
    }
   ],
   "source": [
    "lm=LinearRegression()\n",
    "lm.load_data('./data/linear-regression-train.csv','./data/linear-regression-test.csv')\n",
    "training_error= 0\n",
    "testing_error= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "lm.normalize()\n",
    "beta = lm.train('1')\n",
    "\n",
    "training_error = lm.compute_mse(lm.predict(lm.train_x, beta), lm.train_y)\n",
    "testing_error = lm.compute_mse(lm.predict(lm.test_x, beta), lm.test_y)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training error is: ', training_error)\n",
    "print('Testing error is: ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Ridge regression adds an L2 regularization term to the original objective function. The objective function becomes the following: \n",
    "    $$ J(\\beta) = \\frac{1}{2n} ||X\\beta - Y ||^2 + \\frac{\\lambda}{2n} \\beta^T\\beta ,$$ \n",
    "where $\\lambda \\leq 0$ is a hyper parameter that controls the trade-off. Take the derivative of this provided objective function and derive the new closed form solution for $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://drive.google.com/file/d/1HVBWWUYsW8VAhC9__33toGDcJxoUxCcJ/view?usp=share_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic regression \n",
    "This example will walk you through algorithms for logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (1000, 5)\n",
      "Training labels shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "# As a sanity chech, we print out the size of the training data (1000, 5) and training labels (1000,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Batch gradiend descent\n",
    "In this section, complete the `getBeta_BatchGradient` in `logistic_regression.py`, which computes the gradient of the log likelihoood function. \n",
    "\n",
    "Complete the `compute_avglogL` function in `logistic_regression.py` for sanity check, you should get something around 0.46.\n",
    "\n",
    "To train you model, use `lm.train('0')` function.\n",
    "\n",
    "Compute the training and testing accuracy using `lm.predict` and `lm.compute_accuracy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta shape is: (6,)\n",
      "x shape is: (1000, 6)\n",
      "y shape is: (1000,)\n",
      "average logL for iteration 0: -0.49407742523625303 \t\n",
      "average logL for iteration 1000: -0.46010037535085313 \t\n",
      "average logL for iteration 2000: -0.46010037535085313 \t\n",
      "average logL for iteration 3000: -0.46010037535085313 \t\n",
      "average logL for iteration 4000: -0.46010037535085313 \t\n",
      "average logL for iteration 5000: -0.46010037535085313 \t\n",
      "average logL for iteration 6000: -0.46010037535085313 \t\n",
      "average logL for iteration 7000: -0.46010037535085313 \t\n",
      "average logL for iteration 8000: -0.46010037535085313 \t\n",
      "average logL for iteration 9000: -0.46010037535085313 \t\n",
      "Training avgLogL:  -0.4601003753508529\n",
      "Training accuracy is:  0\n",
      "Testing accuracy is:  0\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "lm.normalize()  # apply z-score normalization to the data\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "lm.normalize()\n",
    "beta = lm.train('0')\n",
    "\n",
    "train_pred = lm.predict(lm.train_x, beta)\n",
    "training_error = lm.compute_accuracy(train_pred, lm.train_y)\n",
    "test_pred = lm.predict(lm.test_x, beta)\n",
    "testing_error = lm.compute_accuracy(test_pred, lm.test_y)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)\n",
    "#logl vals are similar to those on piazza but I dont understand why the accuracies are 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Newton Raphhson\n",
    "In this section, complete the `getBeta_Newton` function in `logistic_regression.py`, which makes use of both first and second derivatives.\n",
    "\n",
    "To train you model, use `lm.train('1')` function.\n",
    "\n",
    "Compute the training and testing accuracy using `lm.predict` and `lm.compute_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/CS145/hw1_2023/hw1code/logistic_regression.py:41: RuntimeWarning: overflow encountered in exp\n",
      "  xit = X[iter].transpose()\n",
      "/home/josh/CS145/hw1_2023/hw1code/logistic_regression.py:29: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(z):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average logL for iteration 0: -inf \t\n",
      "average logL for iteration 500: -inf \t\n",
      "average logL for iteration 1000: -inf \t\n",
      "average logL for iteration 1500: -inf \t\n",
      "average logL for iteration 2000: -inf \t\n",
      "average logL for iteration 2500: -inf \t\n",
      "average logL for iteration 3000: -inf \t\n",
      "average logL for iteration 3500: -inf \t\n",
      "average logL for iteration 4000: -inf \t\n",
      "average logL for iteration 4500: -inf \t\n",
      "average logL for iteration 5000: -inf \t\n",
      "average logL for iteration 5500: -inf \t\n",
      "average logL for iteration 6000: -inf \t\n",
      "average logL for iteration 6500: -inf \t\n",
      "average logL for iteration 7000: -inf \t\n",
      "average logL for iteration 7500: -inf \t\n",
      "average logL for iteration 8000: -inf \t\n",
      "average logL for iteration 8500: -inf \t\n",
      "average logL for iteration 9000: -inf \t\n",
      "average logL for iteration 9500: -inf \t\n",
      "Training avgLogL:  -inf\n",
      "Training accuracy is:  0\n",
      "Testing accuracy is:  0\n"
     ]
    }
   ],
   "source": [
    "lm=LogisticRegression()\n",
    "lm.load_data('./data/logistic-regression-train.csv','./data/logistic-regression-test.csv')\n",
    "training_accuracy= 0\n",
    "testing_accuracy= 0\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "lm.normalize()\n",
    "beta = lm.train('1')\n",
    "\n",
    "train_pred = lm.predict(lm.train_x, beta)\n",
    "training_error = lm.compute_accuracy(train_pred, lm.train_y)\n",
    "test_pred = lm.predict(lm.test_x, beta)\n",
    "testing_error = lm.compute_accuracy(test_pred, lm.test_y)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "print('Testing accuracy is: ', testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "1. Compare the accuracy on the testing dataset for each version. Are they the same? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could not get the functions to work =("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Visualize the decision boundary on a toy dataset\n",
    "\n",
    "In this subsection, you will use the above implementation for another small dataset where each datapoint $x$ only has only two features $(x_1, x_2)$ to visualize the decision boundary of logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (99, 2)\n",
      "Training labels shape: (99,)\n"
     ]
    }
   ],
   "source": [
    "from hw1code.logistic_regression import LogisticRegression\n",
    "\n",
    "lm=LogisticRegression(verbose = False)\n",
    "lm.load_data('./data/logistic-regression-toy.csv','./data/logistic-regression-toy.csv')\n",
    "# As a sanity chech, we print out the size of the training data (99,2) and training labels (99,)\n",
    "print('Training data shape: ', lm.train_x.shape)\n",
    "print('Training labels shape:', lm.train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following block, you can apply the same implementation of logistic regression model (either in 2.1 or 2.2) to the toy dataset. Print out the $\\hat{\\beta}$ after training and accuracy on the train set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta shape is: (3,)\n",
      "x shape is: (99, 3)\n",
      "y shape is: (99,)\n",
      "Training avgLogL:  -0.3291474312957121\n",
      "[-0.04717577  1.46005896  2.06586134]\n",
      "Training accuracy is:  0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "training_accuracy= 0\n",
    "lm.normalize()\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "beta = lm.train('0')\n",
    "\n",
    "training_accuracy = lm.compute_accuracy(lm.predict(lm.train_x, beta), lm.train_y)\n",
    "print(beta)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================# \n",
    "print('Training accuracy is: ', training_accuracy)\n",
    "#why does this work here but not in the previous cells?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try to plot the decision boundary of your learned logistic regression classifier. Generally, a decision boundary is the region of a space in which the output label of a classifier is ambiguous. That is, in the given toy data, given a datapoint $x=(x_1, x_2)$ on the decision boundary, the logistic regression classifier cannot decide whether $y=0$ or $y=1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Is the decision boundary for logistic regression linear? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Yes as the decision boundary where the sigmoid function is equal to .5.  This occurs when the input to the sigmoid function is 0 as e^0 is one so 1/(1+e^0) is 1/2.  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the decision boundary in the following cell. Note that the code to plot the raw data points are given. You may need `plt.plot` function (see [here](https://matplotlib.org/tutorials/introductory/pyplot.html)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [71], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m x1_vec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(lm\u001b[38;5;241m.\u001b[39mtrain_x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmin(),lm\u001b[38;5;241m.\u001b[39mtrain_x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmax(),\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m## x axis of the boundary is also given\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#========================#\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# STRART YOUR CODE HERE  #\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#========================#\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#y = b0 +b1x1 + b2x2\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#since decision boundary at y=0, and we are given x1, we calculate x2 for the decision boundary\u001b[39;00m\n\u001b[1;32m     15\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(x1\u001b[38;5;241m*\u001b[39mbeta[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m beta[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m/\u001b[39mbeta[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot the raw data\n",
    "df = pd.concat([lm.train_x, lm.train_y], axis=1)\n",
    "groups = df.groupby(\"y\")\n",
    "for name, group in groups:\n",
    "    plt.plot(group[\"x1\"], group[\"x2\"], marker=\"o\", linestyle=\"\", label=name)\n",
    "\n",
    "# plot the decision boundary on top of the scattered points\n",
    "x1_vec = np.linspace(lm.train_x[\"x1\"].min(),lm.train_x[\"x1\"].max(),2)  ## x axis of the boundary is also given\n",
    "#========================#\n",
    "# STRART YOUR CODE HERE  #\n",
    "#========================#\n",
    "x1 = np.arange(-2, 2, 0.1)\n",
    "#y = b0 +b1x1 + b2x2\n",
    "#since decision boundary at y=0, and we are given x1, we calculate x2 for the decision boundary\n",
    "x2 = -(x1*beta[1] + beta[0])/beta[2]\n",
    "plt.plot(x1, x2)\n",
    "#========================#\n",
    "#   END YOUR CODE HERE   #\n",
    "#========================#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Homework 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
